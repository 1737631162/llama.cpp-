# 将qwen2.5-1.5B-Instruct模型转换为gguf格式

1. 下载qwen2.5-1.5B-Instruct模型
2. 转为gguf格式
   2.1 `git clone https://github.com/Rayrtfr/llama.cpp.git`
   2.2 `cd llama.cpp`
   2.3 `python convert-hf-to-gguf.py --outfile E:\Model\qwen2.5-1.5B-instruct.gguf E:\Model\Qwen2.5-1.5B-Instruct`

# 将得到的gguf模型进行量化

## 安装make

1.安装[MinGW-w64](https://github.com/niXman/mingw-builds-binaries/releases)
1.1 下载“x86_64-15.1.0-release-win32-seh-ucrt-rt_v12-rev0.7z”  解压  
1.2 将安装路径配置到PATH（解压目录下的bin目录），然后验证 gcc -v

2.安装[make](https://sourceforge.net/projects/gnuwin32/)
2.1 下载make-3.81.exe
2.2 将安装路径配置到PATH（安装目录下的bin目录），然后验证 make -v  

3.安装[w64devkit](https://github.com/skeeto/w64devkit/releases)
3.1 下载“w64devkit-x86-2.3.0.7z.exe”  
3.2 将安装路径配置到PATH（解压目录下的bin目录），然后验证 gcc -v


## windows版
1.llama.cpp目录下执行make命令  
```shell
make clean
make CC=i686-w64-mingw32-gcc CXX=i686-w64-mingw32-g++
```
执行后llama.cpp目录下会生成quantize.exe等执行文件  

2.转换safetensors格式为gguf格式
```shell
python convert-hf-to-gguf.py --outfile llm_ft\LLaMA-Factory\megred-model-path\Qwen-Parameter-Extraction-gguf\Qwen-Parameter-Extraction.gguf llm_ft\LLaMA-Factory\megred-model-path\Qwen-Parameter-Extraction
```

2.使用quantize工具来量化模型
```shell
# 注释：quantize 源文件路径 导出文件路径  量化参数
quantize E:\Model\qwen2.5-1.5B-instruct.gguf E:\Model\qwen2.5-1.5B-instruct-Q4_K_M.gguf Q4_K_M
```

## 量化后部署
[安装ollama运行环境](https://ollama.com/)
1 配置ollama模型存储地址  
2 在配置的ollama模型存储地址目录下新建文件夹  
2.1 放入configuration.json和转换后的qwen2.5-1.5B-instruct.gguf文件  
2.2 新建 Modelfile 文件，写入：`FROM ./qwen2.5-1.5B-instruct.gguf`  
2.3 命令窗口执行 `ollama create qwen2.5:1.5b -f Modelfile`  (OLLAMA_MODELS 系统变量对应的目录下会生成 Ollama 的模型数据)  
2.4 查看模型：`ollama list`  
2.5 运行：`ollama run qwen2.5:1.5b`  
